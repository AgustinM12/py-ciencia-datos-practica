{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85c44d1",
   "metadata": {},
   "source": [
    "### Importaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2e9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cbf4e",
   "metadata": {},
   "source": [
    "### Definir la clase para el agente de DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa309d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    # * constructor de la clase\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # * propiedades de la clase\n",
    "        \n",
    "        # * la cantidad de entradas y salidas que tendra el modelo\n",
    "        # ? numero de entradas (caracteristicas del entorno,velocidad, posición)\n",
    "        self.state_size = state_size\n",
    "        # ? numero de acciones posibles (izquierda o derecha)\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # ? red neuronal principal que hara las predicciones de valores de Q\n",
    "        self.model = self._build_model()\n",
    "        # ? copia congelada de la red que se actualiza periodicamente. Evita las inestabilidades\n",
    "        self.target_model = self._build_model()\n",
    "        # ? actualiza la copia del modelo (copia los pesos de model a target_model)\n",
    "        self.update_target_model()\n",
    "\n",
    "        # ? memoria del modelo donde se guardaran sus experiencias pasadas.\n",
    "        self.memory = []  \n",
    "        self.gamma = 0.95 # ? el valor que le da el agente a las recompensas futuras (entre 0 y 1)\n",
    "        self.epsilon = 1.0 # ? controla la exploración vs explotación (cuánto se arriesga a probar cosas nuevas).\n",
    "        self.epsilon_decay = 0.995 # ? cómo disminuye epsilon en cada episodio para que el agente explore menos y explote más.\n",
    "        self.epsilon_min = 0.01 # ? mínimo valor de epsilon\n",
    "\n",
    "    # * metodo para construir el modelo (red neuronal)\n",
    "    # ? tendra 2 capas ocultas con 24 neuronas\n",
    "    # ? capa final tiene tantas salidas como acciones posibles\n",
    "    # ? utiliza el optimizador \"Adam\" usando MSE (error cuadratico medio)\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    # * elegir que accion se realizara\n",
    "    def act(self, state):\n",
    "        # ? si rand es menor o igual a epsilon actua al hacer\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size) # ? realiza una accion aleatoria.\n",
    "        q_values = self.model.predict(state[np.newaxis], verbose=0) # ? realiza una prediccion.\n",
    "        return np.argmax(q_values[0]) # ? accion con mayor Q (exploracion)\n",
    "\n",
    "    # * guarda las experiencias vividas por el agente para luego usarlas en el entrenamiento.\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # * toma un conjunto aleatorio de experiencias de la memoria\n",
    "    def replay(self, batch_size=32):\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # ? TARGET contiene las predicciones actuales de Q\n",
    "            target = self.model.predict(state[np.newaxis], verbose=0)\n",
    "            if done:\n",
    "                target[0][action] = reward # ? si termino el juego, no hay recompensas a futuro.\n",
    "            else:\n",
    "                # ? si no termino se aplica la ecuacion de Bellman.\n",
    "                t = self.target_model.predict(next_state[np.newaxis], verbose=0)\n",
    "                target[0][action] = reward + self.gamma * np.amax(t[0])\n",
    "            # ? Ajusta la red para que prediga el valor Q actualizado.\n",
    "            self.model.fit(state[np.newaxis], target, epochs=1, verbose=0)\n",
    "\n",
    "        # ? Reduce epsilon gradualmente para que el agente explore menos con el tiempo\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # ? Copia los pesos del modelo principal al modelo objetivo.\n",
    "    # ? Se hace periódicamente (cada ciertos episodios) para mejorar la estabilidad del entrenamiento.\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e8214",
   "metadata": {},
   "source": [
    "### Juego de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be14aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IPF-2025\\Desktop\\MATERIALES\\py-ciencia-datos-practica\\keras-env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intento N° 0\n",
      "Intento N° 1\n",
      "Intento N° 2\n",
      "Intento N° 3\n",
      "Intento N° 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = DQNAgent(state_size=4, action_size=2)\n",
    "\n",
    "for e in range(5):\n",
    "    print(f\"Intento N° {e}\")\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        agent.replay()\n",
    "    agent.update_target_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa562f47",
   "metadata": {},
   "source": [
    "### Explicaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d14f48",
   "metadata": {},
   "source": [
    "#### GYM\n",
    "\n",
    "* **Gym**: (ahora conocido como Gymnasium, anteriormente mantenido por OpenAI) es una librería de Python para simular entornos de **aprendizaje por refuerzo** (RL). Se podria decir que es una **\"sala de entrenamiento\"** para agentes.\n",
    "\n",
    "* Permite\n",
    "1. Probar algoritmos de RL fácilmente.\n",
    "2. Usar entornos listos para entrenar (juegos, robótica, navegación, etc).\n",
    "3. Medir desempeño y compararlo entre algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e097fdf",
   "metadata": {},
   "source": [
    "#### CARTPOLE\n",
    "\n",
    "* Es uno de los entornos más clásicos de RL, incluido en gym:\n",
    "* ¿Como funciona el juego?\n",
    "\n",
    "Hay un carrito sobre un riel, con un palo vertical (como un péndulo invertido).\n",
    "\n",
    "El agente puede mover el carrito izquierda o derecha.\n",
    "\n",
    "El objetivo es mantener el palo en equilibrio vertical el mayor tiempo posible.\n",
    "\n",
    "En resumidas cuentas, es un entorno donde un agente debe equilibrar un palo vertical moviendo un carrito.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107e46d",
   "metadata": {},
   "source": [
    "#### Agente:\n",
    "\n",
    "* **model**: Red neuronal principal que predice los valores Q.\n",
    "* **target_model**: Copia estable usada para calcular valores objetivo.\n",
    "* **memory**: Historial de experiencias para entrenar de forma más estable\n",
    "* **act**: Decide si explorar o explotar una acción\n",
    "* **replay**: Entrena la red usando experiencias guardadas\n",
    "* **epsilon**: Controla cuánto se explora vs se explota, probabilidad de explorar (vs explotar) en cada paso. Representa cuanta curiosidad tiene el agente.\n",
    "* **decaimiento**: Epsilon se reduce para que el agente explore menos a medida que aprende\n",
    "\n",
    "* **exploración**: Prueba nuevas acciones para aprender más del entorno\n",
    "(Quiere probar cosas nuevas (aunque no sepa si son buenas) → eso es explorar.)\n",
    "* **explotación**: Usa el conocimiento actual para elegir la mejor acción conocida\n",
    "(Otras veces prefiere hacer lo que ya sabe que funciona → eso es explotar.)\n",
    "\n",
    "##### Importancia de exploracion y explotacion:\n",
    "* Si el agente solo explota, puede quedarse atascado en una solución subóptima (no descubre opciones mejores).\n",
    "* Si el agente solo explora, nunca usa lo aprendido para mejorar sus resultados.\n",
    "\n",
    "##### Ejemplos de Epsilon:\n",
    "* Si **epsilon = 1.0** el robot hace acciones aleatorias todo el tiempo (explora 100%).\n",
    "*Si **epsilon = 0.0** el robot nunca prueba cosas nuevas, solo hace lo que ya aprendió (explotación total).\n",
    "\n",
    "#### **Q** (quality):\n",
    "Es una estimación de lo bueno que es hacer una acción en una situación.\n",
    "\n",
    "##### Ejemplo del funcionamiento de Q:\n",
    "\n",
    "1. Estado: el palo está inclinado hacia la derecha.\n",
    "\n",
    "2. El robot puede hacer 2 cosas:\n",
    "* Acción 0: mover a la izquierda.\n",
    "* Acción 1: mover a la derecha.\n",
    "\n",
    "3. El modelo estima algo asi:\n",
    "* Q(estado, izquierda) = 0.5\n",
    "* Q(estado, derecha) = 0.9\n",
    "Esto significa que el modelo piensa, \"Si hago 'derecha' ahora, probablemente me irá mejor\".\n",
    "\n",
    "#### Ecuacion de Bellman:\n",
    "Se trata del corazon del aprendizaje por refuerzo, se utiliza para que el agente aprenda a tomar mejores decisiones.\n",
    "\n",
    "\"El valor de estar en un estado y tomar una acción es igual a la recompensa inmediata que obtienes más el valor futuro esperado que obtendrás después\".\n",
    "\n",
    "* Valor presente = recompensa de ahora + valor del futuro\n",
    "\n",
    "##### Utilidades:\n",
    "1. Aprender valores Q de manera inteligente.\n",
    "2. Actualizar el conocimiento en cada paso del agente.\n",
    "3. Guiar las decisiones futuras del agente hacia mejores recompensas.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
